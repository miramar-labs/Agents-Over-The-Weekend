{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S5J_Br6oh5lf"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ[\"LANGSMITH_API_KEY\"] = userdata.get('LANGSMITH_API_KEY')\n",
        "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
        "os.environ[\"LANGSMITH_PROJECT\"] = \"default\"\n",
        "os.environ[\"LANGSMITH_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
        "os.environ[\"LANGSMITH_PROJECT\"] = \"weekend_party\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's load and explore a dataset:"
      ],
      "metadata": {
        "id": "3P5JEl9Ydb6k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "ds = load_dataset(\"cais/mmlu\", \"high_school_geography\")"
      ],
      "metadata": {
        "id": "647VbTYViVHI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds_dict = ds[\"test\"].take(100).to_dict()\n",
        "print(ds_dict[\"question\"][0])"
      ],
      "metadata": {
        "id": "UAAqviFBh_yf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(ds_dict[\"choices\"][0])"
      ],
      "metadata": {
        "id": "4Wmzd6zHdZHi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds_dict[\"answer\"][0]"
      ],
      "metadata": {
        "id": "5Aeaq_lbdaZJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "google_api_key = userdata.get('GOOGLE_API_KEY')\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash-001\", google_api_key=google_api_key)"
      ],
      "metadata": {
        "id": "toV22IlshciE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import load_tools\n",
        "from langgraph.prebuilt import create_react_agent\n",
        "\n",
        "\n",
        "research_tools = load_tools(\n",
        "  tool_names=[\"ddg-search\", \"arxiv\", \"wikipedia\"],\n",
        "  llm=llm\n",
        ")\n",
        "\n",
        "system_prompt = (\n",
        "    \"You're a hard-working, curious and creative student. \"\n",
        "    \"You're working on exam quesion. Think step by step.\"\n",
        "    \"Always provide an argumentation for your answer. \"\n",
        "    \"Do not assume anything, use available tools to search \"\n",
        "    \"for evidence and supporting statements.\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "u2aaOZ6DdfBH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
        "from langgraph.graph import MessagesState\n",
        "from langgraph.prebuilt.chat_agent_executor import AgentState\n",
        "\n",
        "raw_prompt_template = (\n",
        "    \"Answer the following multiple-choice question. \"\n",
        "    \"\\nQUESTION:\\n{question}\\n\\nANSWER OPTIONS:\\n{options}\\n\"\n",
        ")\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [(\"system\", system_prompt),\n",
        "     (\"user\", raw_prompt_template),\n",
        "     (\"placeholder\", \"{messages}\")\n",
        "     ]\n",
        ")\n",
        "\n",
        "class ResearchState(AgentState):\n",
        "  question: str\n",
        "  options: str\n",
        "\n",
        "research_agent = create_react_agent(model=llm, tools=research_tools, state_schema=ResearchState, prompt=prompt)"
      ],
      "metadata": {
        "id": "Y9UqRlvjhfhj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
        "\n",
        "\n",
        "raw_prompt_template_with_critique = (\n",
        "    \"You tried to answer the exam question and you get feedback from your \"\n",
        "    \"professor. Work on improving your answer and incorporating the feedback. \"\n",
        "    \"\\nQUESTION:\\n{question}\\n\\nANSWER OPTIONS:\\n{options}\\n\\n\"\n",
        "    \"INITIAL ANSWER:\\n{answer}\\n\\nFEEDBACK:\\n{feedback}\"\n",
        "\n",
        ")\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [(\"system\", system_prompt),\n",
        "     (\"user\", raw_prompt_template_with_critique),\n",
        "     (\"placeholder\", \"{messages}\")\n",
        "     ]\n",
        ")\n",
        "\n",
        "class ReflectionState(ResearchState):\n",
        "  answer: str\n",
        "  feedback: str\n",
        "\n",
        "research_agent_with_critique = create_react_agent(model=llm, tools=research_tools, state_schema=ReflectionState, prompt=prompt)"
      ],
      "metadata": {
        "id": "IskuJAlwh-94"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Optional\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "\n",
        "reflection_prompt = (\n",
        "    \"You are a university professor and you're supervising a student who is \"\n",
        "    \"working on multiple-choice exam question. \"\n",
        "    \"nQUESTION: {question}.\\nANSWER OPTIONS:\\n{options}\\n.\"\n",
        "    \"STUDENT'S ANSWER:\\n{answer}\\n\"\n",
        "    \"Reflect on the answer and provide a feedback whether the answer \"\n",
        "    \"is right or wrong. If you think the student's answer is correct, rewrite the final answer \"\n",
        "    \"in the `answer` field. \"\n",
        "    \"Only provide critique if you think the asnwer is \"\n",
        "    \"incorrect or there are reasoning flaws. Do not assume anything, \"\n",
        "    \"evaluate only the reasoning the student provided and whether there is \"\n",
        "    \"enough evidence for their answer.\"\n",
        ")\n",
        "\n",
        "class Response(BaseModel):\n",
        "    \"\"\"A final response to the user.\"\"\"\n",
        "\n",
        "    answer: Optional[str] = Field(\n",
        "        description=\"The final answer to the original question. Always provide one if it's right and there's no critique.\",\n",
        "        default=None,\n",
        "    )\n",
        "    critique: Optional[str] = Field(\n",
        "        description=\"A critique of the student's answer. If you think it is incorrect, provide an acitonable feedback\",\n",
        "        default=None,\n",
        "    )\n"
      ],
      "metadata": {
        "id": "SKY-rrWNiA5Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Annotated, Literal, TypedDict\n",
        "from langchain_core.runnables.config import RunnableConfig\n",
        "from operator import add\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langgraph.graph import StateGraph, START, END, Graph\n",
        "\n",
        "\n",
        "class ReflectionAgentState(TypedDict):\n",
        "    question: str\n",
        "    options: str\n",
        "    answer: str\n",
        "    steps: Annotated[int, add]\n",
        "    response: Response\n",
        "\n",
        "\n",
        "def _should_end(state: ReflectionAgentState, config: RunnableConfig) -> Literal[\"research\", END]:\n",
        "    max_reasoning_steps = config[\"configurable\"].get(\"max_reasoning_steps\", 10)\n",
        "    if state.get(\"response\") and state[\"response\"].answer:\n",
        "        return END\n",
        "    if state.get(\"steps\", 1) > max_reasoning_steps:\n",
        "        return END\n",
        "    return \"research\"\n",
        "\n",
        "reflection_chain = PromptTemplate.from_template(reflection_prompt) | llm.with_structured_output(Response)\n",
        "\n",
        "def _reflection_step(state: ReflectionAgentState):\n",
        "    result = reflection_chain.invoke(state)\n",
        "    return {\"response\": result, \"steps\": 1}\n",
        "\n",
        "\n",
        "def _research_start(state: ReflectionAgentState):\n",
        "  answer = research_agent.invoke(state)\n",
        "  return {\"answer\": answer[\"messages\"][-1].content}\n",
        "\n",
        "\n",
        "def _research(state: ReflectionAgentState):\n",
        "  agent_state = {\n",
        "      \"answer\": state[\"answer\"],\n",
        "      \"question\": state[\"question\"],\n",
        "      \"options\": state[\"options\"],\n",
        "      \"feedback\": state[\"response\"].critique\n",
        "  }\n",
        "  answer = research_agent_with_critique.invoke(agent_state)\n",
        "  return {\"answer\": answer[\"messages\"][-1].content}"
      ],
      "metadata": {
        "id": "we3BuKtRiC8K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "builder = StateGraph(ReflectionAgentState)\n",
        "builder.add_node(\"research_start\", _research_start)\n",
        "builder.add_node(\"research\", _research)\n",
        "builder.add_node(\"reflect\", _reflection_step)\n",
        "\n",
        "builder.add_edge(START, \"research_start\")\n",
        "builder.add_edge(\"research_start\", \"reflect\")\n",
        "builder.add_edge(\"research\", \"reflect\")\n",
        "builder.add_conditional_edges(\"reflect\", _should_end)\n",
        "graph = builder.compile()\n",
        "\n",
        "\n",
        "from IPython.display import Image, display\n",
        "display(Image(graph.get_graph().draw_mermaid_png()))"
      ],
      "metadata": {
        "id": "XZTo146KiEL-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's test it out:"
      ],
      "metadata": {
        "id": "dNAmgzqTiF8Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "i = 3\n",
        "question = ds_dict[\"question\"][i]\n",
        "options = \"\\n\".join([f\"{i}. {a}\" for i, a in enumerate(ds_dict[\"choices\"][i])])\n",
        "\n",
        "async for _, event in graph.astream({\"question\": question, \"options\": options}, stream_mode=[\"updates\"]):\n",
        "  print(event)"
      ],
      "metadata": {
        "id": "g0vCUoFEiG6q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = await graph.ainvoke({\"question\": question, \"options\": options})"
      ],
      "metadata": {
        "id": "pux8CUgVkBiZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result"
      ],
      "metadata": {
        "id": "tPk6U4eVkImI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Communication through a shared list of messages"
      ],
      "metadata": {
        "id": "O0vHGvzP2ATy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import load_tools\n",
        "from langgraph.prebuilt import create_react_agent\n",
        "\n",
        "\n",
        "research_tools = load_tools(\n",
        "  tool_names=[\"ddg-search\", \"arxiv\", \"wikipedia\"],\n",
        "  llm=llm\n",
        ")\n",
        "\n",
        "system_prompt = (\n",
        "    \"You're a hard-working, curious and creative student. \"\n",
        "    \"You're working on exam quesion. Think step by step.\"\n",
        "    \"Always provide an argumentation for your answer. \"\n",
        "    \"Do not assume anything, use available tools to search \"\n",
        "    \"for evidence and supporting statements.\"\n",
        ")\n",
        "\n",
        "research_agent = create_react_agent(model=llm, tools=research_tools, prompt=system_prompt)\n",
        "\n",
        "reflection_prompt = (\n",
        "    \"You are a university professor and you're supervising a student who is \"\n",
        "    \"working on multiple-choice exam question. \"\n",
        "    \"Given the dialogue above, reflect on the answer provided and give a feedback \"\n",
        "    \" if needed. If you think the final answer is correct, reply with \"\n",
        "    \"an empty message. Only provide critique if you think the last answer might \"\n",
        "    \"be incorrect or there are reasoning flaws. Do not assume anything, \"\n",
        "    \"evaluate only the reasoning the student provided and whether there is \"\n",
        "    \"enough evidence for their answer.\"\n",
        ")"
      ],
      "metadata": {
        "id": "CeYuGZom2Bu_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "from langgraph.types import Command\n",
        "from langchain_core.runnables import RunnableConfig\n",
        "\n",
        "\n",
        "question_template = PromptTemplate.from_template(\n",
        "    \"QUESTION:\\n{question}\\n\\nANSWER OPTIONS:\\n{options}\\n\\n\"\n",
        ")\n",
        "\n",
        "def _ask_question(state):\n",
        "  return {\"messages\": [(\"human\", question_template.invoke(state).text)]}\n",
        "\n",
        "def _give_feedback(state, config: RunnableConfig):\n",
        "  messages = event[\"messages\"] + [(\"human\", reflection_prompt)]\n",
        "  max_messages = config[\"configurable\"].get(\"max_messages\", 20)\n",
        "\n",
        "  if len(messages) > max_messages:\n",
        "    return Command(\n",
        "      update={},\n",
        "      goto=END\n",
        "    )\n",
        "\n",
        "  result = llm.invoke(messages)\n",
        "\n",
        "  if result.content:\n",
        "    return Command(\n",
        "      update={\"messages\": [\n",
        "          (\"assistant\", result.content),\n",
        "           (\"human\", \"Please, address the feedback above and give an answer.\")]},\n",
        "      goto=\"research\"\n",
        "  )\n",
        "  return Command(\n",
        "      update={},\n",
        "      goto=END\n",
        "  )"
      ],
      "metadata": {
        "id": "6ALtdS8PuHQN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ReflectionAgentStateAlternative(MessagesState):\n",
        "  question: str\n",
        "  options: str\n",
        "\n",
        "\n",
        "builder = StateGraph(ReflectionAgentStateAlternative)\n",
        "builder.add_node(\"ask_question\", _ask_question)\n",
        "builder.add_node(\"research\", research_agent)\n",
        "builder.add_node(\"reflect\", _give_feedback)\n",
        "\n",
        "builder.add_edge(START, \"ask_question\")\n",
        "builder.add_edge(\"ask_question\", \"research\")\n",
        "builder.add_edge(\"research\", \"reflect\")\n",
        "graph = builder.compile()\n",
        "\n",
        "\n",
        "from IPython.display import Image, display\n",
        "display(Image(graph.get_graph().draw_mermaid_png()))"
      ],
      "metadata": {
        "id": "iyi3Dlt0-h-e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "async for _, event in graph.astream({\"question\": question, \"options\": options}, stream_mode=[\"values\"]):\n",
        "  print(len(event[\"messages\"]))"
      ],
      "metadata": {
        "id": "uX3P06j2-krT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for m in event[\"messages\"]:\n",
        "  print(type(m))"
      ],
      "metadata": {
        "id": "UYvbS8tx-nkp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for m in event[\"messages\"]:\n",
        "  m.pretty_print()"
      ],
      "metadata": {
        "id": "nK9F50GY-pu8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installation"
      ],
      "metadata": {
        "id": "GIFsD1mxiB2j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade langsmith langchain-google-genai duckduckgo-search langchain-community langgraph arxiv wikipedia datasets huggingface_hub fsspec"
      ],
      "metadata": {
        "id": "ShR8xiPEiCox"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}