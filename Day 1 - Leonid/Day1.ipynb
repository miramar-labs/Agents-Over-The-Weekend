{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Tracing"
      ],
      "metadata": {
        "id": "1-eAzqwVZeMT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ[\"LANGSMITH_API_KEY\"] = userdata.get('LANGSMITH_API_KEY')\n",
        "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
        "os.environ[\"LANGSMITH_PROJECT\"] = \"default\"\n",
        "os.environ[\"LANGSMITH_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
        "os.environ[\"LANGSMITH_PROJECT\"] = \"weekend_party\""
      ],
      "metadata": {
        "id": "OSmN3fiSZfs4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Getting started"
      ],
      "metadata": {
        "id": "vFY9j5sphPKq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's invoke the Gemini with a simple text input:"
      ],
      "metadata": {
        "id": "eVuy31nOhRZZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EbREhv7fgLAG"
      },
      "outputs": [],
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "google_api_key = userdata.get('GOOGLE_API_KEY')"
      ],
      "metadata": {
        "id": "Rb263mc9g1gR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.0-flash-001\",\n",
        "    google_api_key=google_api_key)"
      ],
      "metadata": {
        "id": "kjYpwP5NgYOa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import Runnable\n",
        "isinstance(llm, Runnable)"
      ],
      "metadata": {
        "id": "rXWF2rhmgm10"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = llm.invoke(\"What is the capital of the USA?\")\n",
        "print(result.content)"
      ],
      "metadata": {
        "id": "CWF1ARONgtNa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result.content"
      ],
      "metadata": {
        "id": "LXQY5QUEg0Fy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(result.usage_metadata)"
      ],
      "metadata": {
        "id": "sk7FS0iahBvf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "user_input = HumanMessage(content=\"What is the capital of the USA?\")"
      ],
      "metadata": {
        "id": "EEnlwE3DhXW2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "step1 = llm.invoke([user_input])"
      ],
      "metadata": {
        "id": "kcGqxRmGhhS_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(step1)"
      ],
      "metadata": {
        "id": "B4hfthOzIzLy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(step1.content)"
      ],
      "metadata": {
        "id": "Q47z6YusI1nI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(step1.usage_metadata)"
      ],
      "metadata": {
        "id": "a1QwT04PI3AF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "prompt_template = (\n",
        "    \"Be concise and answer user's question carefully.\\n\\n\"\n",
        "    \"QUESTION:\\n{question}\\n\"\n",
        ")\n",
        "\n",
        "question = \"What is the capital of the USA?\"\n",
        "lc_prompt_template = PromptTemplate.from_template(prompt_template)\n",
        "lc_prompt_template.invoke({\"question\": question})"
      ],
      "metadata": {
        "id": "_iXIAEMlI7OU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "chain = lc_prompt_template | llm | StrOutputParser()\n",
        "result = chain.invoke({\"question\": question})"
      ],
      "metadata": {
        "id": "ABrQCaQ5JNnh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(result)"
      ],
      "metadata": {
        "id": "xEtkG5tKiNmF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A special placeholder for messages:"
      ],
      "metadata": {
        "id": "Tp51JXL-KORI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate, HumanMessagePromptTemplate\n",
        "from langchain_core.messages import SystemMessage, HumanMessage\n",
        "from langchain_core.prompts import SystemMessagePromptTemplate\n",
        "\n",
        "\n",
        "msg_template = HumanMessagePromptTemplate.from_template(prompt_template)\n",
        "msg_example = msg_template.format(question=question)\n",
        "\n",
        "print(msg_example)\n"
      ],
      "metadata": {
        "id": "Z0DkjU2hKNzO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_prompt_template = ChatPromptTemplate.from_messages([SystemMessage(content=\"You are a helpful assistant.\"), msg_template])\n",
        "chain = chat_prompt_template | llm | StrOutputParser()\n",
        "chain.invoke({\"question\": question})"
      ],
      "metadata": {
        "id": "6xzdD0GKKYXt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "\n",
        "chat_prompt_template = ChatPromptTemplate.from_messages(\n",
        "    [(\"system\", \"You are a helpful assistant.\"),\n",
        "     (\"placeholder\", \"{history}\"),\n",
        "     # same as MessagesPlaceholder(\"history\"),\n",
        "     (\"human\", prompt_template)])"
      ],
      "metadata": {
        "id": "T2ipEzczKfrp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_prompt_template.invoke(question)"
      ],
      "metadata": {
        "id": "h5eUU2TnKh1p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(chat_prompt_template.invoke(question).messages)"
      ],
      "metadata": {
        "id": "EeBas7GtKlj3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(chat_prompt_template.invoke({\"question\": question, \"history\": [(\"user\", \"hi\"), (\"ai\", \"how can I help?\")]}).messages)"
      ],
      "metadata": {
        "id": "VJfrW_jgKqMG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def increment_by_one(x: int) -> int:\n",
        " return x + 1\n",
        "\n",
        "\n",
        "def fake_llm(x: int) -> str:\n",
        " return f\"Result = {x}\""
      ],
      "metadata": {
        "id": "ANH-OhFBK1hx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnableLambda\n",
        "\n",
        "chain = (\n",
        "   increment_by_one | RunnableLambda(fake_llm)\n",
        ")\n",
        "\n",
        "\n",
        "result = chain.invoke(1)\n",
        "print(result)"
      ],
      "metadata": {
        "id": "N9s-IFCgN6ho"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnableSequence\n",
        "\n",
        "\n",
        "a = increment_by_one | RunnableLambda(fake_llm)\n",
        "b = RunnableSequence(RunnableLambda(increment_by_one), RunnableLambda(fake_llm))\n",
        "\n",
        "print(a == b)\n"
      ],
      "metadata": {
        "id": "MqbUgSBKOKwM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.callbacks import UsageMetadataCallbackHandler\n",
        "\n",
        "cb = UsageMetadataCallbackHandler()\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash-001\", google_api_key=google_api_key, callbacks=[cb])"
      ],
      "metadata": {
        "id": "9eL2MLZOOPV8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = lc_prompt_template | llm | StrOutputParser()\n",
        "chain.invoke({\"question\": question})"
      ],
      "metadata": {
        "id": "LuekYPRSRusf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(cb)"
      ],
      "metadata": {
        "id": "4kv0sFJERvft"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Intro to LangGraph"
      ],
      "metadata": {
        "id": "r3ydNlMHr5KH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing_extensions import TypedDict\n",
        "from langchain_core.messages import BaseMessage\n",
        "from langgraph.graph import StateGraph, START, END, Graph\n",
        "\n",
        "class CustomState(TypedDict):\n",
        "    a: int\n",
        "    b: int\n",
        "    result: int\n",
        "\n",
        "\n",
        "def _node_a(state):\n",
        "    return {\"a\": 1}\n",
        "\n",
        "def _node_b(state):\n",
        "    return {\"b\": 2}\n",
        "\n",
        "def _node_sum(state):\n",
        "    a = state[\"a\"]\n",
        "    b = state[\"b\"]\n",
        "    return {\"result\": a+b}\n",
        "\n",
        "builder = StateGraph(CustomState)\n",
        "builder.add_node(\"node_a\", _node_a)\n",
        "builder.add_node(\"node_b\", _node_b)\n",
        "builder.add_node(\"node_sum\", _node_sum)\n",
        "\n",
        "builder.add_edge(START, \"node_a\")\n",
        "builder.add_edge(START, \"node_b\")\n",
        "builder.add_edge(\"node_a\", \"node_sum\")\n",
        "builder.add_edge(\"node_b\", \"node_sum\")\n",
        "builder.add_edge(\"node_sum\", END)\n",
        "\n",
        "workflow = builder.compile()\n",
        "\n",
        "from IPython.display import Image, display\n",
        "display(Image(workflow.get_graph().draw_mermaid_png()))"
      ],
      "metadata": {
        "id": "wXX4tGjfr6xA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for event in workflow.stream({}, stream_mode=\"values\"):\n",
        "  print(event)"
      ],
      "metadata": {
        "id": "13IekX73zFaz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Literal\n",
        "\n",
        "\n",
        "class CustomState(TypedDict):\n",
        "    operation: str\n",
        "    a: int\n",
        "    b: int\n",
        "    result: int\n",
        "\n",
        "def _node_multiply(state):\n",
        "    a = state[\"a\"]\n",
        "    b = state[\"b\"]\n",
        "    return {\"result\": a*b}\n",
        "\n",
        "def _edge(state) -> Literal[\"node_sum\", \"node_multiply\"]:\n",
        "    if state[\"operation\"] == \"sum\":\n",
        "      return \"node_sum\"\n",
        "    return \"node_multiply\"\n",
        "\n",
        "builder = StateGraph(CustomState)\n",
        "builder.add_node(\"node_a\", _node_a)\n",
        "builder.add_node(\"node_b\", _node_b)\n",
        "builder.add_node(\"node_sum\", _node_sum)\n",
        "builder.add_node(\"node_multiply\", _node_multiply)\n",
        "\n",
        "builder.add_edge(START, \"node_a\")\n",
        "builder.add_edge(START, \"node_b\")\n",
        "builder.add_conditional_edges(\"node_a\", _edge)\n",
        "builder.add_conditional_edges(\"node_b\", _edge)\n",
        "builder.add_edge(\"node_sum\", END)\n",
        "builder.add_edge(\"node_multiply\", END)\n",
        "\n",
        "workflow = builder.compile()\n",
        "\n",
        "from IPython.display import Image, display\n",
        "display(Image(workflow.get_graph().draw_mermaid_png()))"
      ],
      "metadata": {
        "id": "D_PpvaYNFEsT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "initial_state = {\"operation\": \"add\"}\n",
        "for event in workflow.stream(initial_state, stream_mode=\"values\"):\n",
        "  print(event)"
      ],
      "metadata": {
        "id": "kcKuzoqzFF8T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for event in workflow.stream({'operation': 'multiply'}, stream_mode=\"values\"):\n",
        "  print(event)"
      ],
      "metadata": {
        "id": "TvPfUigiFaZo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = await workflow.ainvoke(initial_state)\n",
        "print(result)"
      ],
      "metadata": {
        "id": "L55piTzpzbyO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's take at reducers. We saw the default reducer - it replaces the value in the state. Another option is to use a built-in reducer, for example `add` with a list:"
      ],
      "metadata": {
        "id": "_Z47F4MMEclR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "add([1, 2], [3])"
      ],
      "metadata": {
        "id": "n1eJTXNJlu75"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from operator import add\n",
        "from typing import Annotated\n",
        "\n",
        "class CustomState(TypedDict):\n",
        "    values: Annotated[list[int], add]\n",
        "    result: int\n",
        "\n",
        "\n",
        "def _node_a(state):\n",
        "    return {\"values\": [1]}\n",
        "\n",
        "def _node_b(state):\n",
        "    return {\"values\": [2]}\n",
        "\n",
        "def _node_sum(state):\n",
        "    return {\"result\": sum(state[\"values\"])}\n",
        "\n",
        "builder = StateGraph(CustomState)\n",
        "builder.add_node(\"node_a\", _node_a)\n",
        "builder.add_node(\"node_b\", _node_b)\n",
        "builder.add_node(\"node_sum\", _node_sum)\n",
        "\n",
        "builder.add_edge(START, \"node_a\")\n",
        "builder.add_edge(\"node_a\", \"node_b\")\n",
        "builder.add_edge(\"node_b\", \"node_sum\")\n",
        "builder.add_edge(\"node_sum\", END)\n",
        "\n",
        "workflow = builder.compile()\n",
        "\n",
        "from IPython.display import Image, display\n",
        "display(Image(workflow.get_graph().draw_mermaid_png()))"
      ],
      "metadata": {
        "id": "z6v3YakjEMav"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for event in workflow.stream({}, stream_mode=\"values\"):\n",
        "  print(event)"
      ],
      "metadata": {
        "id": "ZELGTJDfGAjv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's take a look at custom reducers:"
      ],
      "metadata": {
        "id": "L11YZ0T0Gmrf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def my_reducer(left: int, right: int) -> int:\n",
        "  if right:\n",
        "    return left + right\n",
        "  return left"
      ],
      "metadata": {
        "id": "BhY8hybpGOBs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from operator import add\n",
        "from typing import Annotated\n",
        "\n",
        "class CustomState(TypedDict):\n",
        "    value: Annotated[int, my_reducer]\n",
        "\n",
        "\n",
        "def _node_a(state):\n",
        "    return {\"value\": 1}\n",
        "\n",
        "def _node_b(state):\n",
        "    return {\"value\": 2}\n",
        "\n",
        "builder = StateGraph(CustomState)\n",
        "builder.add_node(\"node_a\", _node_a)\n",
        "builder.add_node(\"node_b\", _node_b)\n",
        "\n",
        "builder.add_edge(START, \"node_a\")\n",
        "builder.add_edge(\"node_a\", \"node_b\")\n",
        "builder.add_edge(\"node_b\", END)\n",
        "\n",
        "workflow = builder.compile()\n",
        "\n",
        "from IPython.display import Image, display\n",
        "display(Image(workflow.get_graph().draw_mermaid_png()))"
      ],
      "metadata": {
        "id": "DarbOmATGZGA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for event in workflow.stream({}, stream_mode=\"values\"):\n",
        "  print(event)"
      ],
      "metadata": {
        "id": "qEMi_n_SGgNn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "step2 = llm.invoke([user_input, step1, (\"human\", \"How many people live there?\")])"
      ],
      "metadata": {
        "id": "9g2EZH_Xhk8L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(step2.content)"
      ],
      "metadata": {
        "id": "lMJMoHr1hqj1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tracing"
      ],
      "metadata": {
        "id": "Mvnz8m_DMMmU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash-001\", google_api_key=google_api_key)\n",
        "result = llm.invoke(\"What is the capital of the USA?\")\n",
        "print(result.content)"
      ],
      "metadata": {
        "id": "Gfd9iVl7Jr-u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langsmith import traceable\n",
        "\n",
        "@traceable\n",
        "def run():\n",
        "  return llm.invoke(\"What is the capital of the USA?\")"
      ],
      "metadata": {
        "id": "T3baOgBiJ_J5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langsmith import Client, tracing_context, traceable\n",
        "from langsmith.wrappers import wrap_openai\n",
        "\n",
        "langsmith_client = Client(\n",
        "  api_key=userdata.get('LANGSMITH_API_KEY'),\n",
        "  api_url=\"https://api.smith.langchain.com\"\n",
        ")"
      ],
      "metadata": {
        "id": "oyOIhP0sKinK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with tracing_context(enabled=True):\n",
        "  result = llm.invoke(\"What is the capital of the USA?\")"
      ],
      "metadata": {
        "id": "bCLB7583Kum7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = llm.invoke(\"What is the capital of UK?\")"
      ],
      "metadata": {
        "id": "Nb0XzmgdK-K2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using external tools"
      ],
      "metadata": {
        "id": "CKQKlH4Ehs0S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import PromptTemplate\n"
      ],
      "metadata": {
        "id": "k923Q9sIhvGZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's demonstrate how we can instruct an LLM to use an external tool:"
      ],
      "metadata": {
        "id": "JGUb7713i7Bm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "task = (\n",
        "    \"In 1990, the average cost of a gallon of gasoline was $1.16. If the \"\n",
        "    \"inflation rate from 1990 to today has been a cumulative 180%, what would \"\n",
        "    \"that gallon of gas cost in today's money? How does that compare to the \"\n",
        "    \"current average price of gas?\"\n",
        ")\n",
        "\n",
        "raw_prompt_template = (\n",
        "  \"You have access to search engine that provides you an \"\n",
        "  \"information about current events. \"\n",
        "  \"Given the question, decide whether you need an additional \"\n",
        "  \"information from the search engine, and if yes, reply with 'SEARCH: \"\n",
        "   \"<generated query>'. Only if you know enough to answer the user \"\n",
        "   \"then reply with 'RESPONSE <final response>').\\n\"\n",
        "   \"Now, act to answer a user question:\\n{question}\"\n",
        ")\n",
        "prompt_template = PromptTemplate.from_template(raw_prompt_template)\n",
        "\n",
        "response = (prompt_template | llm).invoke(task)\n",
        "print(response.content)"
      ],
      "metadata": {
        "id": "Sh4Wd-G3h4Y1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Technical note: a _PromptTemplate_ allows you substitute variables when executing the chain:"
      ],
      "metadata": {
        "id": "Z3REc5mrkpWH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template.invoke({\"question\": \"TEST\"})"
      ],
      "metadata": {
        "id": "07_b8fCHkup9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"average gas price today\"\n",
        "search_result = \"3.349\""
      ],
      "metadata": {
        "id": "SkYzBWspmWN9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_prompt_template = (\n",
        "  \"You have access to search engine that provides you an \"\n",
        "  \"information about current events. \"\n",
        "  \"Given the question, decide whether you need an additional \"\n",
        "  \"information from the search engine, and if yes, reply with 'SEARCH: \"\n",
        "   \"<generated query>'. Only if you know enough to answer the user \"\n",
        "   \"then reply with 'RESPONSE <final response>').\\n\"\n",
        "   #\"Today is {date}.\"\n",
        "   \"Now, act to answer a user question and \"\n",
        "   \"take into account your previous actions:\\n\"\n",
        "   \"HUMAN: {question}\\n\"\n",
        "   \"AI: SEARCH: {query}\\n\"\n",
        "   \"RESPONSE FROM SEARCH: {search_result}\\n\"\n",
        ")\n",
        "prompt_template = PromptTemplate.from_template(raw_prompt_template)\n",
        "\n",
        "result = (prompt_template | llm).invoke({\"question\": task, \"query\": query, \"search_result\": search_result, \"date\": \"Feb 2025\"})\n",
        "print(result.content)"
      ],
      "metadata": {
        "id": "5K4EDs_XlAYu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating tools with LangChain"
      ],
      "metadata": {
        "id": "jDUNa8RKmoIM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's use a DuckDuckGo search through LangChain:"
      ],
      "metadata": {
        "id": "6DY35rHYm9XS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.tools import DuckDuckGoSearchRun\n",
        "\n",
        "search = DuckDuckGoSearchRun()\n",
        "print(f\"Tool's name = {search.name}\")\n",
        "print(f\"Tool's name = {search.description}\")\n",
        "print(f\"Tool's arg schema = {search.args_schema}\")"
      ],
      "metadata": {
        "id": "z5m-A_wCmpz4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.tools.ddg_search.tool import DDGInput\n",
        "\n",
        "print(DDGInput.model_fields)"
      ],
      "metadata": {
        "id": "UbKbhjk-m8u3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is the weather in Munich like tomorrow?\"\n",
        "search_input = DDGInput(query=query)\n",
        "result = search.invoke(search_input.model_dump())\n",
        "print(result)"
      ],
      "metadata": {
        "id": "LWxbW_j4nFP1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "isinstance(result, str)"
      ],
      "metadata": {
        "id": "xnl266NKnKGd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another example - let's use a web API to instruct an LLM to get the latest information about FX rates:"
      ],
      "metadata": {
        "id": "ZvfwWJ0VnnWL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "api_spec = \"\"\"\n",
        "openapi: 3.0.0\n",
        "info:\n",
        "  title: Frankfurter Currency Exchange API\n",
        "  version: v1\n",
        "  description: API for retrieving currency exchange rates. Pay attention to the base currency and change it if needed.\n",
        "\n",
        "servers:\n",
        "  - url: https://api.frankfurter.dev/v1\n",
        "\n",
        "paths:\n",
        "  /v1/{date}:\n",
        "    get:\n",
        "      summary: Get exchange rates for a specific date.\n",
        "      parameters:\n",
        "        - in: path\n",
        "          name: date\n",
        "          schema:\n",
        "            type: string\n",
        "            pattern: '^\\d{4}-\\d{2}-\\d{2}$' # YYYY-MM-DD format\n",
        "          required: true\n",
        "          description: The date for which to retrieve exchange rates.  Use YYYY-MM-DD format.  Example: 2009-01-04\n",
        "        - in: query\n",
        "          name: symbols\n",
        "          schema:\n",
        "            type: string\n",
        "          description: Comma-separated list of currency symbols to retrieve rates for. Example: GBP,USD,EUR\n",
        "\n",
        "  /v1/latest:\n",
        "    get:\n",
        "      summary: Get the latest exchange rates.\n",
        "      parameters:\n",
        "        - in: query\n",
        "          name: symbols\n",
        "          schema:\n",
        "            type: string\n",
        "          description: Comma-separated list of currency symbols to retrieve rates for. Example: CHF,GBP\n",
        "        - in: query\n",
        "          name: base\n",
        "          schema:\n",
        "            type: string\n",
        "          description: The base currency for the exchange rates. If not provided, EUR is used as a base currency. Example: USD\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "LKDmxcVlnOBe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.agent_toolkits.openapi.toolkit import RequestsToolkit\n",
        "from langchain_community.utilities.requests import TextRequestsWrapper\n",
        "\n",
        "toolkit = RequestsToolkit(\n",
        "    requests_wrapper=TextRequestsWrapper(headers={}),\n",
        "    allow_dangerous_requests=True,\n",
        ")\n",
        "\n",
        "for tool in toolkit.get_tools():\n",
        "  print(tool.name)"
      ],
      "metadata": {
        "id": "QG_Z_-23nv4-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.prebuilt import create_react_agent\n",
        "\n",
        "system_message = (\n",
        "  \"You're given the API spec:\\n{api_spec}\\n\"\n",
        "  \"If possible, use this API if a user asks about foreign exchange rates. \"\n",
        ")\n",
        "\n",
        "agent = create_react_agent(llm, toolkit.get_tools(), prompt=system_message.format(api_spec=api_spec))"
      ],
      "metadata": {
        "id": "rnEVunSun1em"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is the swiss franc to US dollar exchange rate?\"\n",
        "\n",
        "for event in agent.stream({\"messages\": [(\"human\", query)]}, stream_mode=\"values\"):\n",
        "    event[\"messages\"][-1].pretty_print()"
      ],
      "metadata": {
        "id": "6jIQriaOoJrf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = llm.invoke([\n",
        "    (\"system\", system_message.format(api_spec=api_spec)),\n",
        "     (\"human\", \"What is the swiss franc to US dollar exchange rate?\")], tools=toolkit.get_tools())"
      ],
      "metadata": {
        "id": "b1Wk4graoWyJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tool_calls = response.tool_calls\n",
        "print(tool_calls)"
      ],
      "metadata": {
        "id": "HcdnCOyXrkL5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "toolkit.get_tools()[0].run(tool_calls[0][\"args\"])"
      ],
      "metadata": {
        "id": "Ueo6PVMirnCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Defining tools with LangChain"
      ],
      "metadata": {
        "id": "2c-ys0xRGtyb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "from langchain_core.tools import tool\n",
        "import numexpr as ne\n",
        "\n",
        "@tool\n",
        "def calculator(expression: str) -> str:\n",
        "    \"\"\"Calculates a single mathematical expression, incl. complex numbers.\n",
        "\n",
        "    Always add * to operations, examples:\n",
        "      73i -> 73*i\n",
        "      7pi**2 -> 7*pi**2\n",
        "    \"\"\"\n",
        "    math_constants = {\"pi\": math.pi, \"i\": 1j, \"e\": math.exp}\n",
        "    result = ne.evaluate(expression.strip(), local_dict=math_constants)\n",
        "    return str(result)"
      ],
      "metadata": {
        "id": "qi-wbywqGrES"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "calculator.invoke(\"2+2\")"
      ],
      "metadata": {
        "id": "5U61OrHlGqUB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.tools import BaseTool\n",
        "\n",
        "assert isinstance(calculator, BaseTool)\n",
        "print(f\"Tool name: {calculator.name}\")\n",
        "print(f\"Tool name: {calculator.description}\")\n",
        "print(f\"Tool schema: {calculator.args_schema.model_json_schema()}\")"
      ],
      "metadata": {
        "id": "SL1za8A8JVQj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(calculator.args_schema.model_json_schema())"
      ],
      "metadata": {
        "id": "DVjhZ_ZF2Hmp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"How much is 2+3i squared?\"\n",
        "\n",
        "agent = create_react_agent(llm, [calculator])\n",
        "\n",
        "for event in agent.stream({\"messages\": [(\"user\", query)]}, stream_mode=\"values\"):\n",
        "    event[\"messages\"][-1].pretty_print()"
      ],
      "metadata": {
        "id": "4-r-MQ8vJWQD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = (\n",
        "    #\"I ate 200g of chicken breast, 150g of broccoli, and 50g of brown rice for dinner. \"\n",
        "    \"I ate 200g of chicken breast for dinner. \"\n",
        "    \"How many total calories did I consume, and what percentage of my recommended daily \"\n",
        "    \"protein intake does this meal provide if my recommended intake is 75g?\"\n",
        ")\n",
        "\n",
        "system_hint = \"Think step-by-step. Always use search tool to get the fresh information about events or public facts that can change over time. Always use calculator tool for math computations.\"\n",
        "\n",
        "agent = create_react_agent(\n",
        "    llm, [calculator, search],\n",
        "    prompt=system_hint)\n",
        "\n",
        "for event in agent.stream({\"messages\": [(\"user\", question)]}, stream_mode=\"updates\"):\n",
        "    for _, event_values in event.items():\n",
        "      for message in event_values[\"messages\"]:\n",
        "        message.pretty_print()"
      ],
      "metadata": {
        "id": "pn-Rwg_RJpph"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnableLambda, RunnableConfig\n",
        "from langchain_core.tools import tool, convert_runnable_to_tool\n",
        "\n",
        "\n",
        "def calculator(expression: str) -> str:\n",
        "    math_constants = {\"pi\": math.pi, \"i\": 1j, \"e\": math.exp}\n",
        "    result = ne.evaluate(expression.strip(), local_dict=math_constants)\n",
        "    return str(result)\n",
        "\n",
        "calculator_with_retry = RunnableLambda(calculator).with_retry(\n",
        "    wait_exponential_jitter=True,\n",
        "    stop_after_attempt=3,\n",
        ")\n",
        "\n",
        "calculator_tool = convert_runnable_to_tool(\n",
        "    calculator_with_retry,\n",
        "    name=\"calculator\",\n",
        "    description=(\n",
        "        \"Calculates a single mathematical expression, incl. complex numbers.\"\n",
        "        \"'\\nAlways add * to operations, examples:\\n73i -> 73*i\\n\"\n",
        "        \"7pi**2 -> 7*pi**2\"\n",
        "    ),\n",
        "    arg_types={\"expression\": \"str\"},\n",
        ")"
      ],
      "metadata": {
        "id": "iF7KAc3vNUSa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm.invoke(\"How much is (2+3i)**2\", tools=[calculator_tool]).tool_calls[0]"
      ],
      "metadata": {
        "id": "7k6xsDKrNXZz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "calculator_tool.invoke({\"expression\": \"(2+3*i)**2\"})"
      ],
      "metadata": {
        "id": "j4HPKpAVNa4S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent = create_react_agent(llm, [calculator_tool])\n",
        "\n",
        "for event in agent.stream({\"messages\": [(\"user\", \"How much is (2+3i)^2\")]}, stream_mode=\"updates\"):\n",
        "    for _, event_values in event.items():\n",
        "      for message in event_values[\"messages\"]:\n",
        "        message.pretty_print()"
      ],
      "metadata": {
        "id": "GaHzFTmQNsS_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.tools import StructuredTool\n",
        "\n",
        "def calculator(expression: str) -> str:\n",
        "    \"\"\"Calculates a single mathematical expression, incl. complex numbers.\"\"\"\n",
        "    return str(ne.evaluate(expression.strip(), local_dict={}))\n",
        "\n",
        "calculator_tool = StructuredTool.from_function(\n",
        "    func=calculator,\n",
        "    handle_tool_error=True\n",
        ")\n",
        "\n",
        "agent = create_react_agent(llm, [calculator_tool])\n",
        "\n",
        "for event in agent.stream({\"messages\": [(\"user\", \"How much is (2+3i)^2\")]}, stream_mode=\"values\"):\n",
        "    event[\"messages\"][-1].pretty_print()"
      ],
      "metadata": {
        "id": "PGC66SuhNpsL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Controlled generation"
      ],
      "metadata": {
        "id": "rIcYnD7GWVHg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "class Step(BaseModel):\n",
        "    \"\"\"A step that is a part of the plan to solve the task.\"\"\"\n",
        "    step: str = Field(description=\"Description of the step\")\n",
        "\n",
        "class Plan(BaseModel):\n",
        "    \"\"\"A plan to solve the task.\"\"\"\n",
        "    steps: list[Step]\n",
        "\n",
        "\n",
        "prompt = PromptTemplate.from_template(\n",
        "    \"Prepare a step-by-step plan to solve the given task.\\n\"\n",
        "    \"TASK:\\n{task}\\n\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "WxRWvU6mWV_J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm1 = llm.with_structured_output(Plan)"
      ],
      "metadata": {
        "id": "73ZaKWu058Gh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "substituted_prompt = prompt.invoke(\"How to write a bestseller on Amazon about generative AI?\")"
      ],
      "metadata": {
        "id": "_KpPpJUS6GG8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm1.invoke(substituted_prompt)"
      ],
      "metadata": {
        "id": "BzPWj6jC6PAe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = prompt | llm.with_structured_output(Plan)\n",
        "result = chain.invoke(\"How to write a bestseller on Amazon about generative AI?\")\n",
        "assert isinstance(result, Plan)\n",
        "print(f\"Amount of steps: {len(result.steps)}\")\n",
        "for step in result.steps:\n",
        "  print(step.step)\n",
        "  break"
      ],
      "metadata": {
        "id": "Cri0nf3CWoMZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(Plan)"
      ],
      "metadata": {
        "id": "dhsJAMnZ5eX-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for step in result.steps:\n",
        "  print(step.step)"
      ],
      "metadata": {
        "id": "aKu6NeQG5bBs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Plan.model_json_schema()"
      ],
      "metadata": {
        "id": "EK3McXSmWrHA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n"
      ],
      "metadata": {
        "id": "TndH-iMoXjs-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm1 = ChatVertexAI(model_name=\"gemini-1.5-pro-002\", project=\"kuligin-sandbox1\")\n",
        "\n",
        "plan_schema = {\n",
        "    \"type\": \"ARRAY\",\n",
        "    \"items\": {\n",
        "        \"type\": \"OBJECT\",\n",
        "          \"properties\": {\n",
        "              \"step\": {\"type\": \"STRING\"},\n",
        "          },\n",
        "      },\n",
        "}\n",
        "\n",
        "query = \"How to write a bestseller on Amazon about generative AI?\"\n",
        "result = (prompt | llm1.with_structured_output(schema=plan_schema, method=\"json_mode\")).invoke(query)"
      ],
      "metadata": {
        "id": "4-zj59-FX_2h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assert(isinstance(result, list))\n",
        "print(f\"Amount of steps: {len(result)}\")\n",
        "print(result[0])"
      ],
      "metadata": {
        "id": "mrVI4p00YLFq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "llm_json = ChatVertexAI(project=\"kuligin-sandbox1\", model_name=\"gemini-1.5-pro-002\",\n",
        "                        response_mime_type=\"application/json\",\n",
        "                        response_schema=plan_schema)\n",
        "result = (prompt | llm_json | JsonOutputParser()).invoke(query)\n",
        "assert(isinstance(result, list))\n",
        "print(f\"Amount of steps: {len(result)}\")\n",
        "print(result[0])"
      ],
      "metadata": {
        "id": "tbN3DZRjX82A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "response_schema = {\"type\": \"STRING\", \"enum\": [\"positive\", \"negative\", \"neutral\"]}\n",
        "\n",
        "prompt = PromptTemplate.from_template(\n",
        "    \"Classify the tone of the following customer's review:\"\n",
        "    \"\\n{review}\\n\"\n",
        ")\n",
        "\n",
        "review = \"I like this movie!\"\n",
        "llm_enum = ChatVertexAI(project=\"kuligin-sandbox1\", model_name=\"gemini-1.5-pro-002\",\n",
        "                        response_mime_type=\"text/x.enum\",\n",
        "                        response_schema=response_schema)\n",
        "result = (prompt | llm_enum | StrOutputParser()).invoke(review)\n",
        "print(result)"
      ],
      "metadata": {
        "id": "184WuJ7uXxjn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plan_schema = {\n",
        "    \"type\": \"ARRAY\",\n",
        "    \"items\": {\n",
        "        \"type\": \"OBJECT\",\n",
        "          \"properties\": {\n",
        "              \"step\": {\"type\": \"STRING\"},\n",
        "          },\n",
        "      },\n",
        "}\n",
        "\n",
        "query = \"How to write a bestseller on Amazon about generative AI?\"\n",
        "result = (prompt | llm.with_structured_output(schema=plan_schema, method=\"json_mode\")).invoke(query)"
      ],
      "metadata": {
        "id": "sRu-hfd-WzQI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assert(isinstance(result, list))\n",
        "print(f\"Amount of steps: {len(result)}\")\n",
        "print(result[0])"
      ],
      "metadata": {
        "id": "6OXoubH0W1bM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "llm_json = ChatVertexAI(model_name=\"gemini-1.5-pro-002\", response_mime_type=\"application/json\", response_schema=plan_schema)\n",
        "result = (prompt | llm_json | JsonOutputParser()).invoke(query)\n",
        "assert(isinstance(result, list))\n",
        "print(f\"Amount of steps: {len(result)}\")\n",
        "print(result[0])"
      ],
      "metadata": {
        "id": "PUb6rsG5XY2q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_CKcJiGdXbfG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plan-and-solve agent"
      ],
      "metadata": {
        "id": "JaiB_FomQ_RN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pydantic import BaseModel, Field\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "class Plan(BaseModel):\n",
        "    \"\"\"Plan to follow in future\"\"\"\n",
        "\n",
        "    steps: list[str] = Field(\n",
        "        description=\"different steps to follow, should be in sorted order\"\n",
        "    )\n",
        "\n",
        "system_prompt_template = (\n",
        "    \"For the given task, come up with a step by step plan.\\n\"\n",
        "    \"This plan should involve individual tasks, that if executed correctly will \"\n",
        "    \"yield the correct answer. Do not add any superfluous steps.\\n\"\n",
        "    \"The result of the final step should be the final answer. Make sure that each \"\n",
        "    \"step has all the information needed - do not skip steps.\"\n",
        ")\n",
        "planner_prompt = ChatPromptTemplate.from_messages(\n",
        "    [(\"system\", system_prompt_template),\n",
        "     (\"user\", \"Prepare a plan how to solve the following task:\\n{task}\\n\")])\n",
        "\n",
        "planner = planner_prompt | ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.0-flash-001\", temperature=1.0, google_api_key=google_api_key\n",
        ").with_structured_output(Plan)"
      ],
      "metadata": {
        "id": "JqH_u8NHRBt1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "task = \"Write a strategic one-pager of building an AI startup?\"\n",
        "plan = planner.invoke(task)"
      ],
      "metadata": {
        "id": "L0Dh4s8SRNTH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plan"
      ],
      "metadata": {
        "id": "IP0yIVK7bJs6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import load_tools\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash-001\", google_api_key=google_api_key)\n",
        "tools = load_tools(\n",
        "  tool_names=[\"ddg-search\", \"arxiv\", \"wikipedia\"],\n",
        "  llm=llm\n",
        ")"
      ],
      "metadata": {
        "id": "baMKWyBbRWaf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.prebuilt import create_react_agent\n",
        "from langgraph.prebuilt.chat_agent_executor import AgentState\n",
        "from typing_extensions import TypedDict\n",
        "from langgraph.managed import IsLastStep\n",
        "from langgraph.graph.message import add_messages\n",
        "from langchain_core.messages import BaseMessage\n",
        "\n",
        "class StepState(AgentState):\n",
        "  plan: str\n",
        "  step: str\n",
        "  task: str\n",
        "\n",
        "system_prompt = (\n",
        "    \"You're a smart assistant that carefully helps to solve complex tasks.\\n\"\n",
        "    \" Given a general plan to solve a task and a specific step, work on this step. \"\n",
        "    \" Don't assume anything, keep in minds things might change and always try to \"\n",
        "    \"use tools to double-check yourself.\\m\"\n",
        "    \" Use a calculator for mathematical computations, use Search to gather\"\n",
        "    \"for information about common facts, fresh events and news, use Arxiv to get \"\n",
        "    \"ideas on recent research and use Wikipedia for common knowledge.\"\n",
        ")\n",
        "\n",
        "step_template = (\n",
        "    \"Given the task and the plan, try to execute on a specific step of the plan.\\n\"\n",
        "    \"TASK:\\n{task}\\n\\nPLAN:\\n{plan}\\n\\nSTEP TO EXECUTE:\\n{step}\\n\"\n",
        ")\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", system_prompt),\n",
        "    (\"user\", step_template),\n",
        "])\n",
        "\n",
        "execution_agent = create_react_agent(model=llm, tools=tools+[calculator_tool], state_schema=StepState, prompt=prompt_template)"
      ],
      "metadata": {
        "id": "TA1NJIb2RR4o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PlanState(TypedDict):\n",
        "    task: str\n",
        "    plan: Plan\n",
        "    past_steps: Annotated[list[str], add]\n",
        "    final_response: str\n",
        "\n",
        "\n",
        "def get_current_step(state: PlanState) -> int:\n",
        "  \"\"\"Returns the number of current step to be executed.\"\"\"\n",
        "  return len(state.get(\"past_steps\", []))\n",
        "\n",
        "def get_full_plan(state: PlanState) -> str:\n",
        "  \"\"\"Returns formatted plan with step numbers and past results.\"\"\"\n",
        "  full_plan = []\n",
        "  for i, step in enumerate(state[\"plan\"].steps):\n",
        "    full_step = f\"# {i+1}. Planned step: {step}\\n\"\n",
        "    if i < get_current_step(state):\n",
        "      full_step += f\"Result: {state['past_steps'][i]}\\n\"\n",
        "    full_plan.append(full_step)\n",
        "  return \"\\n\".join(full_plan)"
      ],
      "metadata": {
        "id": "gLJE-s-FSJ32"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_prompt = PromptTemplate.from_template(\n",
        "    \"You're a helpful assistant that has executed on a plan.\"\n",
        "    \"Given the results of the execution, prepare the final response.\\n\"\n",
        "    \"Don't assume anything\\nTASK:\\n{task}\\n\\nPLAN WITH RESUlTS:\\n{plan}\\n\"\n",
        "    \"FINAL RESPONSE:\\n\"\n",
        ")\n",
        "\n",
        "async def _build_initial_plan(state: PlanState) -> PlanState:\n",
        "  plan = await planner.ainvoke(state[\"task\"])\n",
        "  return {\"plan\": plan}\n",
        "\n",
        "async def _run_step(state: PlanState) -> PlanState:\n",
        "  plan = state[\"plan\"]\n",
        "  current_step = get_current_step(state)\n",
        "  step = await execution_agent.ainvoke({\"plan\": get_full_plan(state), \"step\": plan.steps[current_step], \"task\": state[\"task\"]})\n",
        "  return {\"past_steps\": [step[\"messages\"][-1].content]}\n",
        "\n",
        "async def _get_final_response(state: PlanState) -> PlanState:\n",
        "  final_response = await (final_prompt | llm).ainvoke({\"task\": state[\"task\"], \"plan\": get_full_plan(state)})\n",
        "  return {\"final_response\": final_response}\n",
        "\n",
        "\n",
        "def _should_continue(state: PlanState) -> Literal[\"run\", \"response\"]:\n",
        "  if get_current_step(state) < len(state[\"plan\"].steps):\n",
        "    return \"run\"\n",
        "  return \"response\"\n",
        "\n",
        "builder = StateGraph(PlanState)\n",
        "builder.add_node(\"initial_plan\", _build_initial_plan)\n",
        "builder.add_node(\"run\", _run_step)\n",
        "builder.add_node(\"response\", _get_final_response)\n",
        "\n",
        "builder.add_edge(START, \"initial_plan\")\n",
        "builder.add_edge(\"initial_plan\", \"run\")\n",
        "builder.add_conditional_edges(\"run\", _should_continue)\n",
        "builder.add_edge(\"response\", END)\n",
        "\n",
        "graph = builder.compile()\n",
        "\n",
        "from IPython.display import Image, display\n",
        "display(Image(graph.get_graph().draw_mermaid_png()))"
      ],
      "metadata": {
        "id": "AZy2UXVXSOsv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image, display\n",
        "display(Image(execution_agent.get_graph().draw_mermaid_png()))"
      ],
      "metadata": {
        "id": "kG28IHL4D_P0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "task = \"Write a strategic one-pager of building an AI startup\"\n",
        "result = await graph.ainvoke({\"task\": task})"
      ],
      "metadata": {
        "id": "Z5P4NhzcSU9S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(result[\"final_response\"].content)"
      ],
      "metadata": {
        "id": "hQzhzs9jTAmQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "async for output in graph.astream({\"task\": task}, stream_mode=\"updates\"):\n",
        "    for key, value in output.items():\n",
        "        print(f\"Output from node '{key}':\")\n",
        "        print(\"---\")\n",
        "        print(value)\n",
        "    print(\"\\n---\\n\")"
      ],
      "metadata": {
        "id": "hquWZOUKSYL1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installation"
      ],
      "metadata": {
        "id": "B32qL0-kgLvZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_google_vertexai langsmith langchain-google-genai duckduckgo-search langchain-community langgraph arxiv wikipedia"
      ],
      "metadata": {
        "id": "MeiGwF8wgNFQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}